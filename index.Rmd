---
title: "Composable State Space Models"
---

```{r, echo=FALSE, warning=FALSE, message=FALSE, background=TRUE, results='hide'}
packages <- c("dplyr", "tidyr", "ggplot2", "gridExtra", "magrittr", "tidyr", "coda", "ggmcmc", "readr")
newPackages <- packages[!(packages %in% as.character(installed.packages()[,"Package"]))]
if(length(newPackages)) install.packages(newPackages)
lapply(packages,require,character.only=T)

theme_set(theme_minimal())
```

This is the documentation for a [Scala library for continuous time partially observed Markov processes](https://git.io/statespace) (POMP). Partially observed Markov processes can be used to model time series data, allowing interpolation and forecasting. 

## Introduction to Partially Observed Markov Process Models

Partially observed Markov processes are a type of [State Space Model](https://en.wikipedia.org/wiki/State-space_representation). This means the models feature unobserved, or latent, variables. The unobserved system state is governed by a [diffusion process](https://en.wikipedia.org/wiki/Diffusion_process), these are continuous time Markov processes meaning that future values of the state space, are independent from all previous values given the current state, x(t). A representation of a POMP model as a directed acyclical graph is below.


```{r dag, engine = "tikz", fig.cap = "Representation of a POMP model as a Directed Acyclic Graph (DAG)", echo=FALSE, fig.align='center'}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{backgrounds}
\usetikzlibrary{positioning}
  \begin{tikzpicture}[node distance = 2cm, auto]
    \tikzstyle{line} = [draw, -latex]
    \tikzstyle{state} = [draw, circle, text width=1.3cm, align=center]
    \tikzstyle{detstate} = [draw, double, circle, text width=1.3cm, align=center]
  
      % Place nodes
      \node (1) {};
      \node [state, right of=1] (2) {$\textbf{x}(t_{i-1})$};
      \node [state, right of=2, node distance=3cm] (3) {$\textbf{x}(t_i)$};
      \node [state, right of=3, node distance=3cm] (4) {$\textbf{x}(t_{i+1})$};
      \node [right of=4, align=center, node distance=2cm] (5) {};
      \node [detstate, below of=2,node distance=3cm] (6) {$\eta(t_{i-1})$};
      \node [detstate, below of=3,node distance=3cm] (7) {$\eta(t_{i})$};
      \node [detstate, below of=4, node distance=3cm] (8) {$\eta(t_{i+1})$};
      \node [state, below of=6, node distance=3cm] (9) {$y(t_{i-1})$};
      \node [state, below of=7, node distance=3cm] (10) {$y(t_i)$};
      \node [state, below of=8, node distance=3cm] (11) {$y(t_{i+1})$};
      
      % Draw edges 
      \path [line] (2) -- (3);
      \path [line] (3) -- (4);
      \path [line] (1) -- (2);
      \path [line] (4) -- (5);
      \path [line] (2) -- (6);
      \path [line] (3) -- (7);
      \path [line] (4) -- (8);
      \path [line] (6) -- (9);
      \path [line] (7) -- (10);
      \path [line] (8) -- (11);
      
      % label arrows
      \node [above right=0.1cm and -0.1cm of 2, node distance=1.5cm] {$p(\textbf{x}(t_i)|\textbf{x}(t_{i-1}))$};
      \node [above right=0.1cm and -0.1cm of 3, node distance=1.5cm] {$p(\textbf{x}(t_{i+1})|\textbf{x}(t_i))$};
      \node [rectangle, below of=2, node distance=1.5cm, fill=white] {$g(F_{t_{i-1}}^T \textbf{x}(t_{i-1}))$};
      \node [rectangle, below of=3, node distance=1.5cm, fill=white] {$g(F_{t_i}^T \textbf{x}(t_i))$};
      \node [rectangle, below of=4, node distance=1.5cm, fill=white] {$g(F_{t_{i+1}}^T \textbf{x}(t_{i+1}))$};
      \node [rectangle, below of=6, node distance=1.5cm, fill=white] {$\pi(y(t_{i-1})|\eta(t_{i-1}))$};
      \node [rectangle, below of=7, node distance=1.5cm, fill=white] {$\pi(y(t_i)|\eta(t_{i}))$};
      \node [rectangle, below of=8, node distance=1.5cm, fill=white] {$\pi(y(t_{i+1})|\eta(t_{i+1}))$};
      
  \end{tikzpicture}
```

The distribution, p, represents the Markov transition kernel of the state space. The distribution pi, represents the observation distribution, parameterised by the state space. The function f is a linear deterministic function, which can be used to add cyclic seasonal components to the state space. The function g is the linking-function from a [generalised linear model](https://en.wikipedia.org/wiki/Generalized_linear_model), which transforms the state space into the parameter space of the observation model. Define $\gamma(t) = F^T_t \textbf{x}(t)$ and $\eta(t) = g(\gamma(t))$.

## Simulating the State Space

An example of a diffusion process is the [Ornstein-Uhlenbeck process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process), which can be simulated by specifying the parameters of the process, `theta`, the mean of the process, `alpha` how quickly the process reverts to the mean and `sigma` the noise of the process. Then we must specify an initial state, which is done by drawing from a Gaussian distribution, since the exact solution to the OU process is a Gaussian distribution. Then we pass a `stepFunction` containing the exact solution to the OU process, relying on only the previous value of the realisation (because the process is  Markovian) and the time difference between realisations.

```scala
import model.StateSpace._
import model.State._
import model.OrnsteinParameter
import model.SimData._
import breeze.stats.distributions.Gaussian

val p = OrnsteinParameter(theta = 1.0, alpha = 0.05, sigma = 1.0)
val initialState = LeafState(DenseVector(Gaussian(6.0, 1.0).draw))

val sims = simSdeStream(initialState, 0.0, 300.0, 1, stepOrnstein(p))
```

Notice, the state space can be multidimensional, and as such is represented by a `Vector`. A single state is represented by a LeafState, this will become clear when considering composition of models. The figure below shows a representation of the Ornstein-Uhlenbeck process with `theta = 6.0, alpha = 0.05, sigma = 1.0`.

```{r ornstein, echo=FALSE, message=FALSE}
system("sbt \"run-main examples.SimulateOrnstein\"")
orn = read_csv("OrnsteinSims.csv", col_names = c("Time", "Value"))

orn %>%
  ggplot(aes(x = Time, y = Value)) + geom_line() +
  ggtitle("Ornstein-Uhleneck Process")
```

## Simulating a Single Model

The observations of a POMP can be from any parameterised distribution. The observation distribution depends on the latent variables and sometimes on additional parameters not in the system state, such as a scaling parameter representing measurement noise. A simple non-gaussian observation model, used for representing count data is the Poisson distribution, parameterised by it's rate $\lambda(t)$. If we consider the rate, $\lambda(t)$ to vary stochastically, then we can represent it using a POMP mode. Firstly select a representation of the state space, we will use the Ornstein-Uhlenbeck process from the previous example, then specify the parameters and the times the process should be observed. There is a `LeafParameter` class which combines the initial State, optional scale parameter and the state space parameters for a single model.


```scala
import model.{LeafParameter, GaussianParameter, OrnsteinParameter}
import model.SimData._
import model.POMP.PoissonModel
import model.StateSpace._

val p = LeafParameter(
  GaussianParameter(-2.0, 1.0),
  None,
  OrnsteinParameter(theta = 2.0, alpha = 1.0, sigma = 1.0))
val mod = PoissonModel(stepOrnstein)


val times = (1.0 to 100.0 by 1.0).toList
val sims = simData(times, mod(p))
```

```{r poisson, echo=FALSE, message=FALSE}
system("sbt \"run-main examples.SimulatePoisson\"")
poisson = read_csv("PoissonSims.csv", col_names = c("Time", "Value", "Eta", "Gamma", "State"))

p1 = poisson %>%
  ggplot(aes(x = Time, y = Value)) + geom_step() + 
  ggtitle("Poisson Observations")

p2 = poisson %>%
  dplyr::select(Time, Eta, State) %>%
  gather(key = "key", value = "value", -Time) %>%
  ggplot(aes(x = Time, y = value, linetype = key)) + geom_line() + 
  facet_wrap(~key, ncol = 1, scales = "free_y") + 
  theme(legend.position="none") + facet_wrap(~key, ncol = 1, scales = "free")

grid.arrange(p1, p2, heights = c(1,2))
```

The figure shows the state space, which varies along the whole real line and the transformed state space and Eta, which varies in $\mathbb{R}^+$ The linking function, g, is the log-link.

## Composing Multiple Models

If we wish to consider more complex process, for instance a Poisson model with a seasonally varying rate, then we have to add deterministic values to the state before applying the observation distribution. The function, f, is a linear deterministic function which can be used to add seasonal factors to the system state. 

Models are represented as a function from `Parameters => Model`, this means models are defined unparameterised. A function for combining two unparameterised models is `Model.combine`, this function is associative, but not commutative. This is because the function selects the leftmost Model's observation and linking functions. The code snippet below shows how to construct a seasonal Poisson model, the observation distribution is Poisson, but the rate of an event occuring follows a daily (period T = 24) cycle if we assume count observations are made once every hour.

```scala
  val poissonParams = LeafParameter(
    GaussianParameter(0.0, 1.0),
    None,
    BrownianParameter(0.1, 0.3))
  val seasonalParams = LeafParameter(
    GaussianParameter(DenseVector(Array.fill(6)(0.0)),
      diag(DenseVector(Array.fill(6)(1.0)))),
    None,
    BrownianParameter(Vector.fill(6)(0.1), Vector.fill(6)(0.4)))

  val params = poissonParams |+| seasonalParams
  val mod = BernoulliModel(stepBrownian) |+| SeasonalModel(24, 3, stepBrownian)

  val times = (1.0 to 100.0 by 1.0).toList
  val sims = simData(times, mod(params))
```

```{r seasonalPoisson, echo=FALSE, message=FALSE}
system("sbt \"run-main examples.SimulateSeasonalPoisson\"")
seasPois = read.csv("seasonalPoissonSims.csv", header = F,
                    col.names = c("Time", "Value", "Eta", "Gamma", sapply(1:7, function(i) paste("State", i, sep = ""))))

p1 = seasPois %>%
  ggplot(aes(x = Time, y = Value)) + geom_step() + 
  ggtitle("Poisson Observations")

p2 = seasPois %>%
  dplyr::select(Time, Eta, Gamma) %>%
  gather(key = "key", value = "value", -Time) %>%
  ggplot(aes(x = Time, y = value, colour = key)) + geom_line() + 
  facet_wrap(~key, ncol = 1, scales = "free_y") + theme(legend.position="none")

p3 = seasPois %>%
  dplyr::select(-Value, -Eta, -Gamma) %>%
  gather(key = "key", value = "value", -Time) %>%
  ggplot(aes(x = Time, y = value, colour = key)) + geom_line() + theme(legend.position = "none")

grid.arrange(p1, p2, p3, heights = c(1,2,1))
```


## Statistical Inference: The Particle Filter

If we have a fully specified model, ie the posterior distributions of the parameters given the data so far are available to us, then we can use a bootstrap particle filter (see [Sequential Monte Carlo Methods in Practice](https://www.springer.com/us/book/9780387951461)) to determine the hidden state space of the observations. Consider the simulated Bernoulli model, the parameters are given by:

```scala
val p = LeafParameter(
  GaussianParameter(6.0, 1.0),
  None,
  OrnsteinParameter(theta = 6.0, alpha = 0.05, sigma = 1.0))
```

The bootstrap particle filter can be applied to the simulated data using a draw from the parameter posterior distribution and the inferred state space can be compared to the previously simulated state space. The data can be read in from a CSV or database, or simulated again. However, since these are stochastic models we can't compare different realisations of the same model. The particle filter is using 1,000 particles.

```scala
  val data = // poisson data

  val p = LeafParameter(
    GaussianParameter(6.0, 1.0),
    None,
    OrnsteinParameter(theta = 6.0, alpha = 0.05, sigma = 1.0))
  
  val mod = BernoulliModel(stepOrnstein)

  // Define the particle filter
  val filter = Filter(mod.model, ParticleFilter.multinomialResampling, data.map(_.t).min)

  // Run the particle filter over the observed data using 1,000 particles
  val filtered = filter.filterWithIntervals(data)(1000)(mod.params)
```

The figure below shows the actual simulated state, plotted next to the estimate state and 99% [credible intervals](https://en.wikipedia.org/wiki/Credible_interval).


```scala
  val data = // poisson data

  val p = LeafParameter(
    GaussianParameter(6.0, 1.0),
    None,
    OrnsteinParameter(theta = 6.0, alpha = 0.05, sigma = 1.0))

  // declare a new filter type including the model, resampling scheme and starting time, t0
  val filter = Filter(mod.model, ParticleFilter.multinomialResampling, t0 = 0.0)
  
  // use the filter to return the state and credible intervals with 1000 particles
  filter.filterWithIntervals(data)(1000)(mod.p)
```

```{r filteredPoisson, echo = FALSE, message=FALSE}
system("sbt \"run-main examples.FilterPoisson\"")
poisson = read_csv("PoissonSims.csv", col_names = c("Time", "Observation", "Eta", "Gamma", "State"))
poissonFiltered = read_csv("PoissonFiltered.csv",
                        col_names = c("Time", "Value", "PredEta", "lowerEta", "upperEta", "PredState", "Lower", "Upper"))

p1 = poisson %>%
  dplyr::select(-Gamma, -Eta, -Observation) %>%
  inner_join(poissonFiltered[,-2], by = "Time") %>%
  select(Time, State, PredState, Lower, Upper) %>%
  gather(key = "key", value = "value", -Time, -Lower, -Upper) %>%
  ggplot() + geom_line(aes(x = Time, y = value, linetype = key)) +
  geom_ribbon(aes(x = Time, ymin = Lower, ymax = Upper), alpha = 0.3)

p2 = poisson %>%
  dplyr::select(-Gamma, -State, -Observation) %>%
  inner_join(poissonFiltered[,-2], by = "Time") %>%
  select(Time, Eta, PredEta, lowerEta, upperEta) %>%
  gather(key = "key", value = "value", -Time, -lowerEta, -upperEta) %>%
  ggplot() + geom_line(aes(x = Time, y = value, linetype = key)) +
  geom_ribbon(aes(x = Time, ymin = lowerEta, ymax = upperEta), alpha = 0.3)

grid.arrange(p1, p2, ncol = 1)
```

## Inference for the Full Joint Posterior Distribution

Say we have observed a time depending process in the real world, and don't have the parameters available for the model. We wish to carry out inference for the state space and the parameters of the model simultaneously. This framework implements the Particle Marginal Metropolis Hastings (PMMH) Algorithm (see [Doucet et al. 2010](http://www.stats.ox.ac.uk/~doucet/andrieu_doucet_holenstein_PMCMC.pdf)). The likelihood of the state space and parameters given the observations can be determined using a particle filter, then a standard Metropolis-Hastings update step is used to create a Markov Chain representing the full join posterior of the model given the observed real-world process.

Now we can implement the PMMH algorithm for the simulated Bernoulli observations, and determine if the algorithm is able to recover the parameters.

```scala
val data = // poisson data

val p = LeafParameter(
  GaussianParameter(6.0, 1.0),
  None,
  OrnsteinParameter(theta = 6.0, alpha = 0.05, sigma = 1.0))

val mod = PoissonModel(stepOrnstein)

// build the particle filter by selecting the model type and resampling scheme
val filter = Filter(mod, ParticleFilter.multinomialResampling, 0.0)

// specify the filter type (llFilter, to return estimate of log-likelihood),
// the number of particles and observations
val mll = filter.llFilter(data)(particles = 1000) _

// build the PMMH algorithm using mll estimate (via particle filter), the
// initial parameters and the proposal distribution for new paramters
val mh = ParticleMetropolis(mll, p, Parameters.perturb(delta))

// run the PMMH as an akka stream in parallel (2 chains) and write the results to a file
runPmmhToFile(s"PoissonSimParams-$delta-$particles", 2, mod.p, mll, Parameters.perturb(0.05), 10000)
```

```{r, echo=FALSE, eval = FALSE}
system("sbt \"run-main examples.GetPoissonParameters 10000 1000 0.05\"")
iters = read_csv("PoissonSimParams-0.05-1000.csv", col_names = c("mu", "sigma", "theta", "alpha", "sigma"))

plotIters = function(iters, variable, thin = 10, burning = nrow(iters)*0.1) {
  mcmcObject = mcmc(iters[seq(from = burnin, to = nrow(iters), by = thin), variable]) %>% ggs()
  
  p1 = ggs_histogram(mcmcObject)
  p2 = ggs_traceplot(mcmcObject)
  p3 = ggs_autocorrelation(mcmcObject)
  p4 = ggs_running(mcmcObject)
  
  grid.arrange(p1, p2, p3, p4)
}

mcmc(iters) %>% plot()
```

Note that the algorithm has been initialised at the same parameter values we used to simulate the model, this kind of prior information is not typically known for real world processes, unless similar processes have been extensively studied. 

## Read More

- Use the navbar at the top, or consult [Simulating from a Model](SimulatingFromAModel.html)
- The [examples](src/main/scala/examples) directory.