<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Streaming MCMC</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #0000ff; } /* Keyword */
code > span.ch { color: #008080; } /* Char */
code > span.st { color: #008080; } /* String */
code > span.co { color: #008000; } /* Comment */
code > span.ot { color: #ff4000; } /* Other */
code > span.al { color: #ff0000; } /* Alert */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #008000; font-weight: bold; } /* Warning */
code > span.cn { } /* Constant */
code > span.sc { color: #008080; } /* SpecialChar */
code > span.vs { color: #008080; } /* VerbatimString */
code > span.ss { color: #008080; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { } /* Variable */
code > span.cf { color: #0000ff; } /* ControlFlow */
code > span.op { } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #ff4000; } /* Preprocessor */
code > span.do { color: #008000; } /* Documentation */
code > span.an { color: #008000; } /* Annotation */
code > span.cv { color: #008000; } /* CommentVar */
code > span.at { } /* Attribute */
code > span.in { color: #008000; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}

.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Composable POMP Models Documentation</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="SimulatingFromAModel.html">Simulating Data</a>
</li>
<li>
  <a href="AddingACustomModel.html">Adding A Model</a>
</li>
<li>
  <a href="ParticleFilter.html">Particle Filter</a>
</li>
<li>
  <a href="StreamingMCMC.html">MCMC</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Streaming MCMC</h1>

</div>

<div id="TOC">
<ul>
<li><a href="#the-pmmh-algorithm">The PMMH Algorithm</a><ul>
<li><a href="#pmmh-as-a-stream">PMMH As A Stream</a></li>
<li><a href="#working-with-a-stream-of-mcmc-iterations">Working with a Stream of MCMC Iterations</a></li>
<li><a href="#pmmh-diagnostics">PMMH Diagnostics</a></li>
</ul></li>
</ul>
</div>

<p>Markov Chain Monte Carlo (MCMC) is a method used to sample from an intractable posterior probability distribution. In general it is used to approximate high-dimensional integrals that can’t be calculated analytically.</p>
<p>In order to estimate the parameters of a Partially Observed Markov Process (POMP) model, we use Particle MCMC methods (see <a href="http://www.stats.ox.ac.uk/~doucet/andrieu_doucet_holenstein_PMCMC.pdf">Doucet et al 2010</a>). This involves estimating the marginal likelihood of the process using a Particle Filter, then using the pseudo-marginal likelihood in a <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">Metropolis hastings</a> algorithm.</p>
<div id="the-pmmh-algorithm" class="section level1">
<h1>The PMMH Algorithm</h1>
<p>The Particle Marginal Metropolis-Hastings algorithm (PMMH)~ is an offline Markov chain Monte Carlo (MCMC) algorithm which targets the full joint posterior <span class="math inline">\(p(\textbf{x}(t_{0:M}), \theta | y(t_{1:M}))\)</span> of a partially observed Markov process. Consider a POMP model given by,</p>
<p><span class="math display">\[
\begin{align*}
Y(t_i)|\eta(t_i) &amp;\sim \pi(y(t_i) | \eta(t_i), \theta), \\
\eta(t_i)|\textbf{x}(t_i) &amp;= g(F^T_{t_i} \textbf{x}(t_i)), \\
\textbf{X}(t_i)|\textbf{x}(t_{i-1}) &amp;\sim p(\textbf{x}(t_i) | \textbf{x}(t_{i-1}), \theta), \quad \textbf{x}(t_0) \sim p(\textbf{x}(t_0)|\theta),
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> represents the parameters to be estimated using the PMMH algorithm. The parameters include the measurement noise in the observation distribution, <span class="math inline">\(\pi(y(t_i) | \eta(t_i), \theta)\)</span>, the parameters of the Markov transition kernel for the system state, <span class="math inline">\(p(\textbf{x}(t_i) | \textbf{x}(t_{i-1}), \theta)\)</span> and the parameters of the initial state distribution <span class="math inline">\(p(\textbf{x}(t_0)|\theta)\)</span>. The data, <span class="math inline">\(y(t)\)</span>, is observed discretely. In order to simulate a Markov chain which targets the full posterior, <span class="math inline">\(p(\theta, \textbf{x} | y)\)</span>, firstly a new set of parameters <span class="math inline">\(\theta^*\)</span> is proposed from a proposal distribution <span class="math inline">\(q(\theta^*|\theta)\)</span>. Then the bootstrap particle filter (see Section~), is run over all of the observed data up to time <span class="math inline">\(t\)</span> using the newly proposed <span class="math inline">\(\theta^*\)</span>. The output of running the filter with the new set of parameters, <span class="math inline">\(\theta^*\)</span> is used to estimate the marginal likelihood, <span class="math inline">\(\hat{p}_{\theta^*}(y) = \prod_{i=1}^n \hat{p}_{\theta^*}(y(t_i)|y(t_{i-1}))\)</span> and, optionally, to sample a new proposed system state, <span class="math inline">\(x^*\)</span> from the conditional distribution <span class="math inline">\(p(x^* | \theta^*, y)\)</span>. The pair <span class="math inline">\((\theta^*, x^*)\)</span> are accepted with probability <span class="math inline">\(\text{min}(1, A)\)</span>, where <span class="math inline">\(A\)</span> is given by:</p>
<p><span class="math display">\[
\begin{equation}
\label{eqn:metropRatio}
A = \frac{p(\theta^*)\hat{p}_{\theta^*}(y) q(\theta^*|\theta)}{p(\theta)\hat{p}_{\theta}(y)q(\theta|\theta^*)},
\end{equation}
\]</span></p>
<p>the distribution <span class="math inline">\(p(\theta)\)</span>, represents the prior distribution over the parameters.</p>
<p>The Metropolis-Hastings Kernel, can be simplified in the case of a symmetric proposal distribution. For a symmetric proposal distribution <span class="math inline">\(q(\theta^*|\theta) = q(\theta|\theta^*)\)</span>. Commonly, the proposal is chosen to be a Normal distribution centered at the previously selected parameter, this is know as a random walk proposal, <span class="math inline">\(q(\theta^*|\theta) = \mathcal{N}(\theta, \sigma)\)</span>, where <span class="math inline">\(\sigma\)</span> is a parameter controlling the step size of the random walk. If a flat prior distribution is chosen, then the ratio can be simplified further to:</p>
<p><span class="math display">\[
\begin{equation*}
A = \frac{\hat{p}_{\theta^*}(y)}{\hat{p}_{\theta}(y)}.
\end{equation*}
\]</span></p>
<p>The full-joint posterior distribution is explored by performing many iterations of the PMMH algorithm, discarding burn-in iterations and possibly thinning the iterations to get less correlated samples from the posterior distribution.</p>
<div id="pmmh-as-a-stream" class="section level2">
<h2>PMMH As A Stream</h2>
<p>The Particle Marginal Metropolis-Hastings algorithm must be applied to a batch of data. Window functions, such as <code>grouped</code>, can be applied to a stream of data to aggregate observations into a batch. <code>grouped</code> accepts an integer, <span class="math inline">\(n\)</span>, and groups each observation into another (finite) stream of size <span class="math inline">\(n\)</span>.</p>
<p>The PMMH algorithm can then be applied to the aggregated group using <code>map</code>. Iterations from the PMMH algorithm are naturally implemented as a stream. In the Scala standard library there is a method for producing infinite streams from an initial seed:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">def</span> iterate[A](start: A)(f: A =&gt; A): Stream[A] </code></pre></div>
<p><code>iterate</code> applies the function <code>f</code> to the starting value, then passes on the result to the next evaluation of <code>f</code>. For example, to create an infinite stream of natural numbers:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> naturalNumbers: Stream[Int] = <span class="fu">iterate</span>(<span class="dv">1</span>)(a =&gt; a + <span class="dv">1</span>)</code></pre></div>
<p>Iterations of an MCMC algorithm can be generated using <code>iterate</code>, by starting with an initial value of the required state (at a minimum the likelihood and the initial set of parameters) and applying the Metropolis-Hastings update at each iteration. Inside of each application of <code>f</code>, a new value of the parameters is proposed, the marginal likelihood is calculated using the new parameters (using the bootstrap particle filter) and the Metropolis-Hastings update is applied.</p>
<p>An illustrative example of a single step in the PMMH algorithm using the Metropolis Kernel is presented in the code block below. Three important functions are given abstract implementations in the <code>MetropolisHastings</code> trait, <code>proposal</code>, <code>prior</code> and <code>logLikelihood</code>. The <code>proposal: Parameters =&gt; Rand[Parameters]</code> is a function representing the (symmetric) proposal distribution, <code>Rand</code> is a representation of a distribution which can be sampled from by calling the method <code>draw</code>. <code>logLikelihood: Parameters =&gt; LogLikelihood</code> is a particle filter, with the observed data and number of particles fixed, which outputs an estimate of the log-likelihood for a given value of the parameters. <code>prior: Parameters =&gt; LogLikelihood</code> represents the prior distribution over the parameters. These three functions will be implemented in a concrete class extending the <code>MetropolisHastings</code> trait and correspond to specific implementation of the PMMH algorithm.</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">trait</span> MetropolisHastings {
    <span class="kw">import</span> MetropolisHastings.<span class="fu">_</span>
    <span class="kw">import</span> math.<span class="fu">_</span>
    <span class="kw">import</span> breeze.<span class="fu">stats</span>.<span class="fu">distributions</span>.{Rand, Uniform}

    <span class="kw">val</span> prior: Parameters =&gt; LogLikelihood
    <span class="kw">val</span> proposal: Parameters =&gt; Rand[Parameters]
    <span class="kw">val</span> logLikelihood: Parameters =&gt; LogLikelihood

    <span class="kw">val</span> stepMetrop: MetropState =&gt; MetropState = s =&gt; {
        <span class="kw">val</span> propParams = <span class="fu">proposal</span>(s.<span class="fu">params</span>).<span class="fu">draw</span>
        <span class="kw">val</span> propll = <span class="fu">logLikelihood</span>(propParams)
        <span class="kw">val</span> a = propll + <span class="fu">prior</span>(propParams) - s.<span class="fu">ll</span> - <span class="fu">prior</span>(s.<span class="fu">params</span>)

        <span class="kw">if</span> (<span class="fu">log</span>(<span class="fu">Uniform</span>(<span class="dv">0</span>, <span class="dv">1</span>).<span class="fu">draw</span>) &lt; a)
            <span class="fu">MetropState</span>(propll, propParams)
        <span class="kw">else</span> s
    }
}

<span class="kw">object</span> MetropolisHastings {
    <span class="kw">type</span> Parameters = Vector[Double]
    <span class="kw">type</span> LogLikelihood = Double

    <span class="kw">case</span> <span class="kw">class</span> <span class="fu">MetropState</span>(ll: LogLikelihood, params: Parameters) 
}</code></pre></div>
<p>In order to generate a stream of iterations, use <code>iterate</code>:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> initState = <span class="fu">MetropState</span>(-<span class="fl">1e99</span>, initParams)
<span class="kw">val</span> iters = <span class="fu">iterate</span>(initState)(stepMetrop)</code></pre></div>
<p>Where <code>initParams</code> are drawn from the prior distribution and the initial value of the log-likelihood is chosen to be very small so the first iteration of the PMMH is accepted.</p>
<p>Built in stream operations can be used to discard burn-in iterations and thin the iterations to reduce auto-correlation between samples. The stream can be written to a file or database at each iteration, so the PMMH algorithm implemented as a stream uses constant memory as the chain size increases.</p>
</div>
<div id="working-with-a-stream-of-mcmc-iterations" class="section level2">
<h2>Working with a Stream of MCMC Iterations</h2>
<p>By default, the iterations from the PMMH algorithm are an Akka stream. Since the parameters are a stream, we can perform streaming operations on parameters, such as thinning:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">def</span> thinParameters[A](thin: Int) = {
    Flow[A].<span class="fu">zip</span>(Source(Stream.<span class="fu">from</span>(<span class="dv">1</span>))).
      filter{ <span class="kw">case</span> (_, iteration) =&gt; iteration % thin == <span class="dv">0</span>}.
      map{ <span class="kw">case</span> (p, _) =&gt; p }
  }</code></pre></div>
<p>The stream of parameters is zipped to a stream of integers, <code>1 2 3 ...</code>, and then filtered, finally the integers are removed by a call to map. MCMC algorithms often take a little while to properly start exploring the posterior, these iterations can be discarded by calling <code>drop</code>.</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">def</span> burnin[A](burn: Int) = {
    Flow[A].<span class="fu">drop</span>(burn)
  }</code></pre></div>
<p>Sometimes a complex model with many free parameters can require a large amount of iterations from the PMMH algorithm to determine the parameter posterior. In this case, we would like a way to stream the iterations to a file in order to avoid running out of memory. With Akka streams, we can asynchronously write to a file using the <code>fileIO</code> sink:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">params.
  map{ p =&gt; <span class="fu">ByteString</span>(s<span class="st">&quot;$p</span><span class="ch">\n</span><span class="st">&quot;</span>) }.
  <span class="fu">runWith</span>(FileIO.<span class="fu">toFile</span>(<span class="kw">new</span> File(<span class="st">&quot;output.csv&quot;</span>)))</code></pre></div>
<p>A <code>toString</code> method in the <code>Parameters</code> class gives the CSV output. In order to determine the state space online using the parameter posterior, a suitable summary of the parameters must be provided for use the with [[particle filter|The-Particle-Filter]]. The mean of the parameters is a suitable value.</p>
<p>First, consider how to calculate the average of parameters from a file. Since the iterations of the PMMH algorithm are written to a file, we must read in and parse the parameters. Consider a simple linear model:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">  <span class="kw">val</span> p = <span class="fu">LeafParameter</span>(<span class="fu">GaussianParameter</span>(<span class="fl">3.0</span>, <span class="fl">2.0</span>), Some(<span class="fl">1.0</span>), <span class="fu">BrownianParameter</span>(<span class="fl">0.1</span>, <span class="fl">1.0</span>))
  <span class="kw">val</span> mod = <span class="fu">LinearModel</span>(stepBrownian)

  <span class="kw">val</span> times = (<span class="fl">0.0</span> to <span class="fl">50.0</span> by <span class="fl">0.5</span>).<span class="fu">toList</span>
  <span class="kw">val</span> sims = <span class="fu">simData</span>(times, <span class="fu">mod</span>(p))</code></pre></div>
<p>Now we have simulated data from a linear model, with a Gaussian observation model, let’s recover the parameters using Particle Marginal Metropolis Hastings:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> mll = <span class="fu">pfMll</span>(sims, mod)(<span class="dv">500</span>)
<span class="fu">ParticleMetropolis</span>(mll, p, Parameters.<span class="fu">perturb</span>(<span class="fl">1.0</span>)).<span class="fu">iters</span>.
  <span class="fu">take</span>(<span class="dv">10000</span>).
  <span class="fu">map</span>( p =&gt; <span class="fu">ByteString</span>(s<span class="st">&quot;$p</span><span class="ch">\n</span><span class="st">&quot;</span>)).
  <span class="fu">runWith</span>(FileIO.<span class="fu">toFile</span>(<span class="kw">new</span> File(<span class="st">&quot;LinearMCMC.csv&quot;</span>)))</code></pre></div>
<p>Now we can read back in the file <code>LinearMCMC.csv</code>, parse, drop burnin terms and thin the stream before calculating the mean:</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala">  <span class="kw">val</span> future = <span class="fu">cleanParameterFlow</span>(<span class="kw">new</span> File(<span class="st">&quot;LinearMCMC.csv&quot;</span>), burnin = <span class="dv">1000</span>, thin = <span class="dv">2</span>, totalIterations = <span class="dv">10000</span>)
  <span class="kw">val</span> p: Vector[Double] = Await.<span class="fu">result</span>(p.<span class="fu">run</span>, <span class="dv">1</span> second)
  <span class="kw">val</span> params = Parameter(<span class="fu">GaussianParameter</span>(p.<span class="fu">head</span>, <span class="fu">p</span>(<span class="dv">1</span>)), Some(<span class="fu">p</span>(<span class="dv">2</span>)), <span class="fu">BrownianParameter</span>(<span class="fu">p</span>(<span class="dv">3</span>), <span class="fu">p</span>(<span class="dv">4</span>)))</code></pre></div>
<p><code>cleanParameterFlow</code> returns a future, since it could be reading an unbounded file. So we force it to return a result and parse the vector to the set of parameters required for the Linear Model.</p>
</div>
<div id="pmmh-diagnostics" class="section level2">
<h2>PMMH Diagnostics</h2>
<p>In practice it is desirable to run multiple chains from different starting values (possibly with different prior distributions), with different proposal distributions or with different number of particles in order to establish whether the chain will converge on the correct posterior distribution. Since individual chains are independent, we can run them in parallel, which is easy using <a href="https://akka.io">Akka Streams</a>.</p>
<div class="sourceCode"><pre class="sourceCode scala"><code class="sourceCode scala"><span class="kw">val</span> params = <span class="co">// initial parameters</span>
<span class="kw">val</span> mll = <span class="co">// a particle filter, function from Parameters =&gt; LogLikelihood</span>
<span class="kw">val</span> particles = Vector(<span class="dv">100</span>, <span class="dv">200</span>, <span class="dv">500</span>, <span class="dv">1000</span>)
<span class="kw">val</span> iterations = <span class="dv">10000</span>

Source(particles).
  <span class="fu">mapAsync</span>(parallism = <span class="dv">4</span>).
  map { p =&gt; 

    <span class="co">// initialise a PMMH algorithm with different amounts of particles</span>
    <span class="kw">val</span> mh = <span class="fu">ParticleMetropolis</span>(<span class="fu">mll</span>(particles), params, Parameters.<span class="fu">perturb</span>(<span class="fl">0.1</span>))
    <span class="fu">runMcmc</span>(mh, s<span class="st">&quot;exampleMCMC-$particles&quot;</span>, iterations).<span class="fu">run</span>()
    
  }.
  <span class="fu">runWith</span>(Sink.<span class="fu">onComplete</span> { _ =&gt;
    system.<span class="fu">terminate</span>()
  })</code></pre></div>
<p>A full suite of diagnostic tools can be found in the R package <a href="https://cran.r-project.org/web/packages/coda/index.html">coda</a>. Since the MCMC iterations are written to a csv, this can easily be imported using R and analysed using coda.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
